{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Ki6jcwCjGwk1"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"Eo4S8f58Gwk9"},"source":["# Word Embeddings\n","## Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYyDkHMSGwk_"},"outputs":[],"source":["# First, you'll need to install gensim\n","# !pip install gensim\n","\n","# Import the necessary modules\n","\n","from gensim.test.utils import common_texts\n","\n","from gensim.models import Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0FCHyZ0GwlB","outputId":"0cd84126-f12a-45aa-8302-077208ba2a5c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692155504934,"user_tz":240,"elapsed":4,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"]}],"source":["print(common_texts) #Sample Data"]},{"cell_type":"markdown","metadata":{"id":"obAnQvieGwlE"},"source":[" Word2vec accepts several parameters that affect both training speed and quality.\n","\n","One of them is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them:\n","1\n","\n","model = Word2Vec(sentences, min_count=10)  # default value is 5\n","\n","A reasonable value for min_count is between 0-100, depending on the size of your dataset.\n","\n","Another parameter is the size of the NN layers, which correspond to the “degrees” of freedom the training algorithm has:\n","1\n","\n","model = Word2Vec(sentences, vector_size=200)  # default value is 100\n","\n","Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds."]},{"cell_type":"markdown","source":["Other hyper-parameters:\n","\n","*   size: window=window_size for capturing context for target word\n","\n","*   sample: The threshold for configuring which higher-frequency words are randomly down sampled, useful range is (0, 1e-5)\n","\n","*   workers: Use these many worker threads to train the model (faster training with multicore machines)\n","\n","*   sg: Training algorithm: skip-gram if sg=1, otherwise CBOW.\n","\n","*   iter: Number of iterations (epochs) over the corpus.\n"],"metadata":{"id":"eHJa7t_dVlNi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"enTdB5hPGwlH"},"outputs":[],"source":["model = Word2Vec(sentences=common_texts, vector_size=10, window=5, min_count=1, workers=4)\n","#Here, vector_size = 10 denotes the length of embedding\n","model.save(\"word2vec.model\")"]},{"cell_type":"markdown","metadata":{"id":"yiL5GEqaGwlJ"},"source":["If you save the model you can continue training it later:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EVn5-eMTGwlK"},"outputs":[],"source":["# load the saved model\n","model = Word2Vec.load(\"word2vec.model\")\n","# model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)"]},{"cell_type":"markdown","metadata":{"id":"bcwKsP0OGwlM"},"source":["The trained word vectors are stored in a KeyedVectors instance, as model.wv:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQ-S-El4GwlO","outputId":"9e3d002a-da9f-474d-82d7-093ac4707bc0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692155600673,"user_tz":240,"elapsed":142,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[-0.00410223 -0.08368949 -0.05600012  0.07104538  0.0335254   0.0722567\n","  0.06800248  0.07530741 -0.03789154 -0.00561806]\n","10\n"]}],"source":["# Get the embeddings for the word 'human'\n","embedding = model.wv['human']\n","\n","print(embedding)\n","print(len(embedding))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zWtQqtrGwlP","outputId":"7278e032-ba1c-4e0a-bab1-a48065627dd5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692155615679,"user_tz":240,"elapsed":167,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[('graph', 0.3586882948875427), ('system', 0.22743132710456848), ('time', 0.1153423935174942)]\n"]}],"source":["# Get the most similar words (having the most similar embeddings)\n","similar_words = model.wv.most_similar('human',topn = 3) #topn denotes the top 3 similar words\n","print(similar_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NrllmXJqGwlR"},"outputs":[],"source":["# Store just the words + their trained embeddings.\n","word_vectors = model.wv\n","word_vectors.save(\"word2vec.wordvectors\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFj9xhGgGwlS","outputId":"aaf2f75a-9d28-41ea-d7fa-bb48b922b610","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692155635419,"user_tz":240,"elapsed":141,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.0163195 ,  0.00189972,  0.03474648,  0.00217841,  0.09621626,\n","        0.05062076, -0.08919986, -0.0704361 ,  0.00901718,  0.06394394],\n","      dtype=float32)"]},"metadata":{},"execution_count":11}],"source":["# Load back with memory-mapping = read-only, shared across processes.\n","from gensim.models import KeyedVectors\n","wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n","wv['computer']  # Get numpy vector embedding for 'computer'"]},{"cell_type":"markdown","metadata":{"id":"1Hme83l4GwlT"},"source":["### Refer to the link below for more details:\n","https://radimrehurek.com/gensim/models/word2vec.html"]},{"cell_type":"markdown","metadata":{"id":"kM6M_oYAGwlU"},"source":["# Gensim comes with several already pre-trained models, in the Gensim-data repository"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WEvTubVQGwlV","outputId":"7ead5196-7a6f-43cd-e94d-4d120ae8242f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694408284317,"user_tz":-330,"elapsed":542,"user":{"displayName":"Sanatan Sukhija","userId":"00318259453802622356"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"]}],"source":["import gensim.downloader\n","# Show all available models in gensim-data\n","print(list(gensim.downloader.info()['models'].keys()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0gl5eMeCGwlV","outputId":"50d98e4c-f083-483b-fd2b-be7b7166a925","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692155882530,"user_tz":240,"elapsed":63482,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[=================================================-] 99.8% 104.6/104.8MB downloaded"]},{"output_type":"execute_result","data":{"text/plain":["<gensim.models.keyedvectors.KeyedVectors at 0x7b3c30b4ae90>"]},"metadata":{},"execution_count":13}],"source":["# Download the \"glove-twitter-25\" embeddings\n","# Pre-trained glove vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased.\n","glove_vectors = gensim.downloader.load('glove-twitter-25')\n","glove_vectors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CcG_GXoUGwlX","outputId":"28b2f508-ffbe-403f-e03b-787da3584499","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692155882749,"user_tz":240,"elapsed":222,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('facebook', 0.948005199432373),\n"," ('tweet', 0.9403423070907593),\n"," ('fb', 0.9342358708381653),\n"," ('instagram', 0.9104824066162109),\n"," ('chat', 0.8964964747428894),\n"," ('hashtag', 0.8885937333106995),\n"," ('tweets', 0.8878158330917358),\n"," ('tl', 0.8778461217880249),\n"," ('link', 0.8778210878372192),\n"," ('internet', 0.8753897547721863)]"]},"metadata":{},"execution_count":14}],"source":["# Use the downloaded vectors as usual:\n","glove_vectors.most_similar('twitter')"]},{"cell_type":"markdown","metadata":{"id":"WvnW9NgBGwla"},"source":["# Document/Sentence Embeddings\n","Paragraph, Sentence, and Document embeddings\n","\n","## Doc2vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"If7cxdRzGwlj","executionInfo":{"status":"ok","timestamp":1692156445538,"user_tz":240,"elapsed":164,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"}},"outputId":"c15016e7-dbe1-4b51-9cb9-dceb2368280e","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence Embeddings:\n","[array([ 0.03662118, -0.03340049,  0.03493926,  0.02863173, -0.02873892,\n","        0.0061017 , -0.00928018,  0.020404  ,  0.01168683, -0.04370352],\n","      dtype=float32), array([-0.0264247 ,  0.01556579, -0.02829748,  0.02891731, -0.03690882,\n","       -0.00761334, -0.02413336, -0.00400685,  0.03742655, -0.04779235],\n","      dtype=float32), array([ 0.01470128, -0.03377239, -0.01528271,  0.03292239, -0.0388649 ,\n","        0.01191715, -0.00158414,  0.00560945, -0.0179112 ,  0.03335478],\n","      dtype=float32), array([-1.8668937e-02,  1.6141423e-03,  1.8629819e-02,  1.3088283e-02,\n","        3.4870736e-02,  2.5650885e-02,  4.2355411e-02, -7.3844306e-03,\n","       -2.1560920e-02, -8.4493120e-05], dtype=float32), array([-0.01476743,  0.03778674, -0.00343251,  0.04148247,  0.01027184,\n","       -0.02704399,  0.01868414,  0.01600945,  0.00645805, -0.02633146],\n","      dtype=float32)]\n","\n","Shape:\n","(5, 10)\n"]}],"source":["from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","\n","# Define your sentences (example)\n","sentences = [\"this is the first sentence\", \"this is the second sentence\", \"yet another sentence\", \"one more sentence\", \"and the final sentence\"]\n","\n","# Tag the sentences for training\n","tagged_data = [TaggedDocument(words=sentence.split(), tags=[str(i)]) for i, sentence in enumerate(sentences)]\n","\n","# Train the model\n","model = Doc2Vec(tagged_data, vector_size=10, window=2, min_count=1, workers=4)\n","\n","# Get the embeddings for the sentences\n","sentence_vectors = [model.infer_vector(sentence.split()) for sentence in sentences]\n","# The infer_vectors expects the input as a list of words (nltk.word_tokenize())\n","\n","print(\"Sentence Embeddings:\")\n","print(sentence_vectors) #Embeddings of the sentences\n","\n","import numpy as np\n","print(\"\\nShape:\")\n","print(np.array(sentence_vectors).shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bF1RI8k8Gwlk","outputId":"bbccb793-f486-47f8-ba90-4520d0e8ec99","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692156448718,"user_tz":240,"elapsed":134,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.03662118, -0.03340049,  0.03493926,  0.02863173, -0.02873892,\n","        0.0061017 , -0.00928018,  0.020404  ,  0.01168683, -0.04370352],\n","      dtype=float32)"]},"metadata":{},"execution_count":16}],"source":["sentence_vectors[0] #the first embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-XesFtYhGwll","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692156449950,"user_tz":240,"elapsed":320,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"}},"outputId":"b73d6d28-042c-4fc8-b084-1d37be21dddd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.06275015"]},"metadata":{},"execution_count":17}],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","cosine_similarity(sentence_vectors[1].reshape(1,-1),sentence_vectors[2].reshape(1,-1))[0][0]\n","#Cosine similarity between embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0DKoWDb-Gwln","outputId":"188e0a01-8e36-4c75-d85e-2b411c23e8f2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692156450585,"user_tz":240,"elapsed":2,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1.0000001 ,  0.25135136,  0.25508478, -0.21339901,  0.02778555],\n","       [ 0.25135136,  1.        , -0.06275015, -0.4493906 ,  0.45510653],\n","       [ 0.25508478, -0.06275015,  1.        , -0.17689413, -0.30497724],\n","       [-0.21339901, -0.4493906 , -0.17689413,  1.        ,  0.19173679],\n","       [ 0.02778555,  0.45510653, -0.30497724,  0.19173679,  1.        ]],\n","      dtype=float32)"]},"metadata":{},"execution_count":18}],"source":["# Find the similarity between all the sentences\n","similarity = cosine_similarity(sentence_vectors)\n","similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H_8s2ff2Gwlo","outputId":"888230df-eaae-486f-9d85-55674ce436d0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692156455378,"user_tz":240,"elapsed":149,"user":{"displayName":"Ahmed Elbagoury","userId":"00894633619817385555"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Input Sentence --> this is the first sentence\n","Most Similar Sentence --> yet another sentence\n","Cosine Simialrity: 0.25508475\n"]}],"source":["#Find the most similar sentence to the first sentence (at index = 0)\n","ind = 0  # The index of the sentence for which you want to find the most similar sentence\n","max = -1 # This will store the cosine_similarity of the most similar document\n","print(\"Input Sentence -->\", sentences[ind])\n","for i in range(np.array(sentence_vectors).shape[0]):\n","    if i != ind:\n","        if max < cosine_similarity(sentence_vectors[i].reshape(1,-1),sentence_vectors[ind].reshape(1,-1))[0][0]:\n","            max = cosine_similarity(sentence_vectors[i].reshape(1,-1),sentence_vectors[ind].reshape(1,-1))[0][0]\n","            s_ind = i\n","\n","print(\"Most Similar Sentence -->\", sentences[s_ind])\n","print(\"Cosine Simialrity:\", max)"]},{"cell_type":"markdown","metadata":{"id":"tZynnc0BGwlp"},"source":["#### More about Doc2vec here:\n","https://radimrehurek.com/gensim/models/doc2vec.html"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}