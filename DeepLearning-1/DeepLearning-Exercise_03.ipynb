{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1zTcXZJCy_A9TPftYNSDJpkEVtMZh2MVr","timestamp":1723777114679},{"file_id":"1lALD9WwwmS8fltZ0dg3aaA559MwjLeif","timestamp":1634761316278}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WlzUBjlgazPB"},"source":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"4OSGXCcKUqN_"}},{"cell_type":"markdown","metadata":{"id":"7rQMEE1PDxui"},"source":["**NER prediction with LSTM and Transformers**"]},{"cell_type":"code","source":[],"metadata":{"id":"xbfr2JbWUDwt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4viZuySLD3HD"},"source":["For this assignment we will be exploring the use of lstms and transformers for named entity recognition (NER) tasks. In this case, we will be looking at recognizing word tagging (e.g., classifying each word as a business, a place, etc...)"]},{"cell_type":"markdown","metadata":{"id":"0z8dD6pSEXDm"},"source":["First, download and upload the ner_dataset.csv file from this site (https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus?select=ner_dataset.csv), we will be using this for experiments."]},{"cell_type":"markdown","metadata":{"id":"K-Zvwl90EnZJ"},"source":["Import the libraries we will need."]},{"cell_type":"code","metadata":{"id":"_wFWdYWwEqFp"},"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from itertools import chain\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vRyutDB9E0sX"},"source":["Let's look at the structure of the data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":330},"id":"yNj70CojE3Ve","executionInfo":{"elapsed":188,"status":"error","timestamp":1716515974398,"user":{"displayName":"Aniruddha Basak","userId":"05179242321558275563"},"user_tz":420},"outputId":"62ad803b-209d-41e0-9c6e-99110d762104"},"source":["data = pd.read_csv('ner_dataset.csv', encoding= 'unicode_escape')\n","data.head(10)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'ner_dataset.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-72e36543b354>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ner_dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'unicode_escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ner_dataset.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"-jvynmAHFjKx"},"source":["We next need to create a mapping between tokens, tags, and ids. Each token should map to a unique id, and each tag should map to a unique class."]},{"cell_type":"markdown","metadata":{"id":"kSg1neW0IdnD"},"source":["Now you might have noticed that each sentece is split into multiple rows.\n","\n","1.   List item\n","2.   List item\n","\n","We need to transform this data into sequences of words and tags."]},{"cell_type":"code","metadata":{"id":"ibNA2AqWZp4Q"},"source":["# Fill na\n","data_fillna = data.fillna(method='ffill', axis=0)\n","# Groupby and collect columns\n","data_group = data_fillna.groupby(\n","['Sentence #'],as_index=False\n",")['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n","# Visualise data\n","data_group.head(10).\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"Jske89drZgXA"}},{"cell_type":"markdown","metadata":{"id":"4oBPyo8ZJDPm"},"source":["Next we split the data into training and testing"]},{"cell_type":"code","metadata":{"id":"xoT2bYY1ZsNg"},"source":["#Enter your code here\n","# sample data\n","['thousands of demonstrators were in Hyde park', 'thousands', 'O']\n","# ['thousands of demonstrators were in Hyde park', 'of', 'O']\n","['thousands of demonstrators were in Hyde park', 'demonstrators', 'O']\n","\n","['thousands of demonstrators were in Hyde park', 'Hyde', 'A-geo']\n","['thousands of demonstrators were in Hyde park', 'Park', 'B-geo']\n","\n","# X, y\n","['thousands of demonstrators were in Hyde park', 'thousands'] ['O']\n","['thousands of demonstrators were in Hyde park', 'Hyde'] ['A-geo']\n","\n","# 100 samples of class 'O', 10 from 'A-geo', 10 from 'B-geo'\n","# y - can take a few values. M classes\n","# Multi class classification problem\n","# CLass imbalance: O is super popular\n","# Train M binary classifiers, each predicting i th class vs all\n","# training data for class 'A-geo' : 10 vs 110\n","# Need to down sample majority class\n","[0.2, 0.1, 0.7]"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define Metrics\n","# Class specific PR => PR, ROC AUC"],"metadata":{"id":"eLfmjjDPaALU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gkZ6PauuaoXx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Encoding:\n","## Goal:\n","'thousands of demonstrators were in Hyde park' =>\n","[0.2, 1.1, 3.0, 0.4, ...0.9]\n","\n","## Approaches:\n","\n","\n","1.   Word encoding: create a vocabulary. enumerate them\n","10K - 20K words.\n","\n","thousands => 345\n","thousand => 999\n","demonstrators => 12\n","demonstrate => 671\n","demonstrating => 910\n","\n","one hot encoding for each token\n","[]\n","10 * 10K element sparse vector\n","\n","2.   Word piece encoding\n","thousand => thou san d' '\n","thousands => thou san ds' '\n","repeat the same one-hot encoding\n","\n","hundreds\n","\n","3. Byte pair encoding\n","\n","\n"],"metadata":{"id":"3ayR8S9Maot5"}},{"cell_type":"code","source":[],"metadata":{"id":"HKC2MdNseQbi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5AcJAYFEeQYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["['thousands of demonstrators were in Hyde park', 'thousands'] ['O'] =>\n","[10, 31, 671, ..., 0]\n","[10, 31, 671, ..., 2]"],"metadata":{"id":"--BLfccdeLlQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TPBi9B3vKPfy"},"source":["**Next** create the LSTM model"]},{"cell_type":"markdown","source":[],"metadata":{"id":"Dxh27TQqeTdU"}},{"cell_type":"code","source":["pre_loaded_embeddings = {'sentence': [0.1,0.3]}\n","def get_embedding(sentence):\n","  unknown_sentence_embedding = [0,0,0]\n","  e = pre_loaded_embeddings.get(sentence,\n","                                   unknown_sentence_embedding)\n","  return torch.tensor(e)"],"metadata":{"id":"WVwzmoQXfD0o"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kg2UnsT0KRfx"},"source":["# Multi class classification model\n","class LSTMNerModel(nn.Module):\n","  def __init__(self,embedding_dim, num_tags, hidden_dim=64):\n","    super().__init__()\n","    self.embedding = get_embedding\n","    self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim)\n","    self.linear = nn.Linear(in_features=hidden_dim, out_features=num_tags)\n","    self.soft_max = nn.functional.log_softmax # (in, dim=1)\n","\n","  def forward(self, sentence, word_index):\n","    embeddings = self.embedding(sentence)\n","    lstm_out, lstm_hidden = self.lstm(embeddings)\n","    # decide which lstm output to use\n","    # one option: pick out the word_index\n","    y = lstm_out[:, word_index]\n","    y = self.linear(y)\n","    tag_scores = self.soft_max(y, dim=1)\n","    return tag_scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"joMehFqIK67E"},"source":["Define the loss function for the task"]},{"cell_type":"code","metadata":{"id":"Fh3V2M_eK9Un"},"source":["# Enter code here\n","# CELoss => Binary CE Loss\n","learning_rate = 0.001\n","epochs = 100\n","model = LSTMNerModel(...)\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# tags should be of the same form as the tag_scores\n","# [0, 0, 1]\n","for epoch in range(epochs):\n","  for sentence, word_position, tags in training_data:\n","    tag_scores = model.forward(sentence, word_position)\n","    # tag_scores = model(sentence, word_position)\n","    loss = loss_function(tag_scores, tags)\n","\n","    model.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    # compute metrics\n","    # plot convergence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"eKtXoWvhfrdL"}},{"cell_type":"markdown","metadata":{"id":"6IKXPtCLLKf9"},"source":["Train the model. First, find some pre-trained embeddings to help us with the task...for example, you can find GloVe embeddings here https://nlp.stanford.edu/projects/glove/"]},{"cell_type":"code","metadata":{"id":"stbwKuR5LPd9"},"source":["def load_embeddings():\n","  lines = open(\"glove.6B.100d.txt\", \"r\").readlines()\n","\n","  w2e = {}\n","  for l in lines:\n","    s = l.split(\" \")\n","    word = s[0]\n","    embedding = np.zeros( (1, len(s)-1))\n","    for k, x in enumerate(s[1:]):\n","      embedding[0,k] = float(x.strip())\n","\n","    w2e[word] = embedding\n","\n","  return w2e\n","\n","w2e = load_embeddings()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKui8Vdu74GG"},"source":["# Enter Code here"],"execution_count":null,"outputs":[]}]}