{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_NecYSoU9RX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Description :**\n",
        "Suppose you are a data scientist working for a lending institution that wants to improve its loan approval process.The institutions has provided you with a dataset containing information about loan issued by LendingClub, an online lending platform.\n",
        "\n",
        "Your task is to analyze the dataset and develop an machine learning model that can accurately predict whether a loan application should be approved or not.\n",
        "\n",
        "By accurately predicting loan approval outcomes, your model will assist the learning institution in making a informed decisions, reducing the risk of default and improving the overall loan portfolio performance.\n",
        "\n",
        "This will not only streamline the loan approval process but also enable the institution to better assess the creditworthiness of the loan applicants,lending to more sound lending practices."
      ],
      "metadata": {
        "id": "1I6RwqCadWuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the Libraries"
      ],
      "metadata": {
        "id": "Uhkrv-Fye5P8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload the data into colab"
      ],
      "metadata": {
        "id": "Tv0-VaXSfB_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Introduction: Analyse the features that we got from the data\n",
        "\n",
        "1. Check the columns of the dataset\n",
        "2. check the shape of the dataset\n",
        "3. check the data types of features in the dataset using info\n",
        "4. Check the missing values of the data\n",
        "5. Check the percentage of the missing values in the dataset\n",
        "6. Chekc the unique value of loan status\n",
        "7. Check the summary statistics for numerical columns in the dataset\n",
        "Missing values can have a significant impact on the performance and reliability of the machine learning models.\n",
        "\n",
        "For this problem - we can keep only those features that have less than of 20% missing values"
      ],
      "metadata": {
        "id": "lcwfzLgSXR_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDA - Exploratory Data Analysis - to gain the insight of dataset\n",
        "\n",
        "Visualize the data, calculate summary statistics , and identify any pattern or trends.\n",
        "\n",
        "1. Calculating or checking if there is any missing value for features in the data.\n",
        "2. Check if we need to perform imputation ( filling missing values) - this is a common approach to handle missing data in ML.-\n",
        "            1.   Mean / Median / Mode Imputations\n",
        "            2.   Forward Fill / Backward Fill\n",
        "            3.   Regression Imputation etc.\n",
        "\n",
        "\n",
        "3. Plot the visualize the loan status count.\n",
        "\n",
        "4. Plot Loan status count vs length of employment\n",
        "\n",
        "5. **Perform Correlation** - Correlation is a statistical measure that quantifies the relationship between two variables. It measures the extent to which changes in one variable are assoicated with changes in another variable. Correaltion is commonly used in the data analysis to understand the strength and direction of the relationship between the variables.\n",
        "\n",
        "  **Positive Correlation** - A high positive correalation indicates that as the feature increases, the other one also tend to increase as well.\n",
        "\n",
        "  **Negative Correlation** - A high negative correlation indicates that as the feature increases, the other one also tend to decrease as well.\n",
        "\n",
        "6. Divide the feature into categorical and numerical - we need to convert categorical to numerical if we want to use those categorical feature into ML model.\n",
        "\n",
        "7. Plot the data into histogram - A histogram is a graphical representation of the distribution of the dataset. It displays the frequency or counts of observation within different intervals or bins. A histogram can provide insights into underlying distribution of a continuous variable and help identify patterns, outliers, or skewness in the data.\n",
        "\n",
        "8. We make a list of numerical features and plot histogram of that features to look into their distribution.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "upKTuqgrff1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocessing** -\n",
        "Preprocess the dataset by handling missing values, addressing categorical variables, and performing any necessary data transformations. This step is to ensure that the data is in suitable format for ML alogorithms.\n",
        "\n",
        "1. Change the data type from string into numerical - example - term - '36 months' to '36'\n",
        "\n",
        "2. **Label Encoding** - is a technique used to convert the categorical variables into numerical transformation. In ML , algorithms typically require numerical inputs, so label encoding is employed to transoform categorical data into a format that can be easily understood by the algorithm .\n",
        "\n",
        "3. **One-hot Encoding**- is a technique used to convert categorical variables into a binary matrix format, where each category becomes a separate binary features. It is commonly used in ML to represent the categorical data in a way that is compatiable with various algorithms ."
      ],
      "metadata": {
        "id": "can8rey4lBFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Split the dataset into training and testing sets for model evaluation**\n",
        "\n",
        "1. Training dataset only with features which is X and target feature as y\n",
        "\n",
        "2. Dividing the dataset into independent and dependent features .\n",
        "\n",
        "3. Splitting the dataset between training and testing set\n",
        "\n",
        "4. Transforming the dataset - For this problem , we are using minmax scalar to normalize the feature. The **MinMaxScalar** is common feature scaling technique used in ML . It scales features to a specific range , typically between 0 and 1. The main reason for using MinMaxScalar is to ensure that all features have the same scale and to bring the data within a specific range.\n",
        "\n",
        "As we saw from the distribution plot , the data has various range and the distribution is different . We are mapping all those into a single range."
      ],
      "metadata": {
        "id": "HNQx4ss4m3EF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model**\n",
        "\n",
        "1. Train and evaluate various classification mdoels ,such as Logistic regression, SVM etc\n",
        "\n",
        "2. Compare the performance of these models to identify the the most accurate one for loan approval predictions\n",
        "\n",
        "3. Fine-tune the selection model by adjusting hyperparameters. Use technique like regularization ."
      ],
      "metadata": {
        "id": "JL7aF7CboQ2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression\n",
        "\n",
        "1. Declare the model\n",
        "2. Fit a logistic regression model to the training data.\n",
        "3. Predict the model on the testing data.\n",
        "4. Compute the confusion matrix and classification report of the model\n",
        "            1.   Accuracy\n",
        "            2.   Precision\n",
        "            3.   Recall\n",
        "            4.   F1-Score\n",
        "            5.   Support\n",
        "\n"
      ],
      "metadata": {
        "id": "JRkH2Rcro9dt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regularization - it is used when we want to improve the model.\n",
        "\n",
        "Here , we are using l2 regularization technique with an extra parameter i.e. solver .  \n",
        "\n",
        "**L2 regularization or Ridge regularization**  is a commonly used technique in ML to **prevent overfitting and improve the generalization performance** of the model. It accomplishes this by adding a penalty term to the loss function during the training process.\n",
        "\n",
        "**Solver** is nothing but an optimization technique.Different solvers employ different optimization technique to minimize the cost function or maximize the likelihood function associated with logistic regression . The choice of solver can impact the efficiency and accuracy of the model's training process.\n",
        "\n",
        "Try more parameters"
      ],
      "metadata": {
        "id": "GjoFNMQ7sHfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polinomial Features\n",
        "\n",
        "1. Create a pipeline with polynomial features and logistics regression\n",
        "\n",
        "2. Fit into logistic regression model with penalty term, iteration and solver\n",
        "\n",
        "3. Fit the model on the training dataset\n",
        "\n",
        "4. Calculate the accuracy or confusion matrix"
      ],
      "metadata": {
        "id": "M_ZAL7tx0Mqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support Vector Machine ( SVM) -\n",
        "\n",
        "1. Fit SVM model\n",
        "2. Training the model with our training data with a simple parameter C . C is the regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.The penalty is a squared l2 penalty.\n",
        "\n",
        "kernel - Specifies the kernel type to be used in the algorithm\n",
        "degree - Degree of the polynomial kernel function . must be non negative\n",
        "gamma - Kernel coefficient for 'rbf' ,'poly' and 'sigmoid'"
      ],
      "metadata": {
        "id": "Z2lMoP3j07U5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K Nearest Neighbour - KNN\n",
        "\n",
        "1. Fit KNN model\n",
        "\n",
        "n_neighbors = usually between 3 to 10 . default is 5\n",
        "\n",
        "Algorithm = algorithm used to compute nearest neighbors - ball_tree , kd_tree,brute, auto"
      ],
      "metadata": {
        "id": "yUN96IvF2O5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gaussian Naive Bayes\n",
        "\n",
        "1. Fit Naive Bayes model\n",
        "\n",
        "2. Fit Bernoulli Naive Bayes model"
      ],
      "metadata": {
        "id": "9a0epHe-3H3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper Parameter using gridsearchcv\n",
        "\n",
        "Finding the best combination of hyperparameters for a machine learning model . It performs an exhaustive search over a predefined grid of hyperparameter values and evaluate the model's performance for each combination.\n",
        "\n",
        "1. Define the hyperparameters to tune and the range of values to try\n",
        "\n",
        "2. Create a Bernoulli Naive Bayes model\n",
        "\n",
        "3. Use GridSearchCV to find the best hyperparameters\n",
        "\n",
        "4. Print the best hyperparameters\n",
        "\n",
        "5. Use the best hyperparameters to fit the model on the training data."
      ],
      "metadata": {
        "id": "7OtZG2GR3f1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion -\n",
        "\n",
        "Assess the model's performance using appropriate evaluation metrics , such as accuracy , precision , recall, and F1-score. This evaluation will provide insights into how well the model can predict loan approval outcomes. Provide insights and recommendations based on the model's predictions and feature importance. Analyze the impact of the different features on the loan approavl decision and identify key factors that significantly influence the outcomes.\n",
        "\n",
        "Additionally, prepare a comprehensive report summarizing your findings and recommendations for improving the loabn approavl process based on the insights gained from the development model.\n",
        "\n",
        "Note - Ensure that you apply suitable feature engineering techniques , and use appropriate validation strategies to ensure reliable modle performance . Additionally , consider the ethical implications of the mnodel's prediction and address any biases that may arise during the analysis."
      ],
      "metadata": {
        "id": "6zU3FDOj4c61"
      }
    }
  ]
}